{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"w0esik6wl4fI"},"outputs":[],"source":["%pip install captum\n","%matplotlib inline\n","%pip install grad-cam\n","%pip install resampy"]},{"cell_type":"code","source":["import numpy as np\n","import json\n","import os\n","from captum.attr import IntegratedGradients\n","from captum.attr import LayerConductance\n","from captum.attr import NeuronConductance\n","import captum.attr\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import torch\n","import torchvision\n","from torchvision import transforms\n","from torchvision import models\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm, tqdm_notebook\n","from torch.autograd import Variable\n","from torch.autograd import Function\n","from fastprogress.fastprogress import format_time, master_bar, progress_bar\n","from sklearn.metrics import f1_score, jaccard_score\n","from sklearn import preprocessing\n","from scipy import stats\n","import pandas as pd\n","import resampy"],"metadata":{"id":"Qvhd7PaFpAzv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Yy7_R0tupbA6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class InstrumentDataset(Dataset):\n","  def __init__(self, csv_file, json_file, root_dir, spec_type):\n","    self.audio_frame = pd.read_csv(csv_file)\n","    with open(json_file, 'r') as f:\n","      self.instrument_classes = json.load(f)\n","    \n","    self.json_file = json_file\n","    self.csv_file = csv_file\n","    self.root_dir = root_dir\n","\n","    # Add chroma_cqt if desired\n","    if spec_type not in [\"mel_spectrogram\", \"chroma_stft\", \"cqt\"]:\n","      raise Exception(\"Not valid spectrogram type\")\n","    else:\n","      self.spec_type = spec_type\n","\n","    # List of unique sample_keys (aka audio file names)\n","    self.unique_audio_files = self.audio_frame.sample_key.unique()\n","\n","    # Dataframe specifying the sample_key of the audio file and the instrument labels\n","    self.audio_file_labels = self.audio_frame.groupby('sample_key')['instrument'].apply(list).reset_index(name='labels')\n","\n","    # self.audio_file = self.audio_file_labels.iloc[:500,:].copy()\n","    self.audio_file = self.audio_file_labels.copy()\n","\n","    num_data = self.audio_file.shape\n","\n","    os_join = np.vectorize(os.path.join)\n","\n","    self.audio_file[spec_type] = np.full(num_data[0], [0])\n","    self.audio_file[spec_type] = os_join(np.full(num_data[0], self.root_dir), np.full(num_data[0], spec_type),\n","                                          self.audio_file.sample_key.str[:3], \n","                                          self.audio_file.sample_key.str[:] + np.full(num_data[0], '_' + spec_type + '.npy'))\n","\n","    # Matrix of instrument labels ordered by audio file number (increasing sample_key value)\n","    # self.label_matrix = self.audio_file_labels.labels.tolist()\n","    self.label_matrix = self.audio_file.labels.tolist()\n","\n","    binarizer = preprocessing.MultiLabelBinarizer()\n","    \n","    self.binary_label_matrix = binarizer.fit_transform(self.label_matrix)\n","    self.label_df = pd.DataFrame(self.binary_label_matrix,columns=[instrument for instrument in self.instrument_classes.keys()])\n","\n","    self.audio_file = pd.concat([self.audio_file, self.label_df], axis=1)\n","\n","  def get_instrument_class_dict(self):\n","    return self.instrument_classes\n","\n","  def __len__(self):\n","    return len(self.audio_file.index)\n","\n","  def __getitem__(self, idx):\n","    # Allow for slicing\n","    if torch.is_tensor(idx):\n","      idx = idx.tolist()\n","\n","    if type(idx) is int:\n","      idx = [idx]\n","\n","    # Get the instruments types on hot encoded\n","    instrument_types = np.array(self.audio_file.iloc[idx, 3:]).astype(float)\n","    # Get the spectrograms as a numpy array from the npy files\n","    specs = np.array(self.audio_file.iloc[idx, 2])\n","\n","    spec_transforms = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize([-47.3835, -47.3835, -47.3835], [18.5056, 18.5056, 18.5056])\n","    ])\n","    \n","    for file_idx, file_name in enumerate(specs):\n","      # 3 channel expanded\n","      spec = np.repeat(np.expand_dims(np.load(file_name).astype(float),-1), 3, -1)\n","      # 1 channel\n","      # spec = np.load(file_name).astype(float)\n","      # spec = 2*(spec - np.min(spec))/np.ptp(spec) - 1\n","      spec = spec_transforms(spec)\n","      specs[file_idx] = spec\n","\n","    specs = np.stack(specs)\n","    specs = np.squeeze(specs)\n","    # specs = np.repeat(specs[:,:,:,np.newaxis], 3, -1)\n","    # print(specs.shape)\n","\n","    instrument_types = torch.from_numpy(instrument_types)\n","\n","    \n","    sample = {'specs': specs, \n","              'instrument(s)': instrument_types, \n","              'sample_key': self.audio_file.iloc[idx, 0].tolist(), \n","              'spec_type': self.spec_type}\n","\n","    return sample"],"metadata":{"id":"Z6Z5posOp8-C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class VGGishInstrumentDataset(Dataset):\n","  def __init__(self, csv_file, json_file, root_dir, spec_type):\n","    self.audio_frame = pd.read_csv(csv_file)\n","    with open(json_file, 'r') as f:\n","      self.instrument_classes = json.load(f)\n","    \n","    self.json_file = json_file\n","    self.csv_file = csv_file\n","    self.root_dir = root_dir\n","\n","    # Add chroma_cqt if desired\n","    if spec_type not in [\"vgg\", \"audio\"]:\n","      raise Exception(\"Not valid spectrogram type\")\n","    else:\n","      self.spec_type = spec_type\n","\n","    # List of unique sample_keys (aka audio file names)\n","    self.unique_audio_files = self.audio_frame.sample_key.unique()\n","\n","    # Dataframe specifying the sample_key of the audio file and the instrument labels\n","    self.audio_file_labels = self.audio_frame.groupby('sample_key')['instrument'].apply(list).reset_index(name='labels')\n","\n","#     self.audio_file = self.audio_file_labels.iloc[:500,:].copy()\n","    self.audio_file = self.audio_file_labels.copy()\n","\n","    num_data = self.audio_file.shape\n","\n","    os_join = np.vectorize(os.path.join)\n","\n","    self.audio_file[spec_type] = np.full(num_data[0], [0])\n","#     self.audio_file[spec_type] = os_join(np.full(num_data[0], self.root_dir), np.full(num_data[0], 'audio'), np.full(num_data[0], 'audio'),\n","#                                           self.audio_file.sample_key.str[:3], \n","#                                           self.audio_file.sample_key.str[:] + np.full(num_data[0], '.ogg'))\n","    self.audio_file[spec_type] = os_join(np.full(num_data[0], self.root_dir), np.full(num_data[0], self.spec_type),\n","                                          self.audio_file.sample_key.str[:3], \n","                                          self.audio_file.sample_key.str[:] + np.full(num_data[0], '_' + spec_type + '.npy'))\n","\n","    # Matrix of instrument labels ordered by audio file number (increasing sample_key value)\n","    # self.label_matrix = self.audio_file_labels.labels.tolist()\n","    self.label_matrix = self.audio_file.labels.tolist()\n","\n","    binarizer = preprocessing.MultiLabelBinarizer()\n","    \n","    self.binary_label_matrix = binarizer.fit_transform(self.label_matrix)\n","    self.label_df = pd.DataFrame(self.binary_label_matrix,columns=[instrument for instrument in self.instrument_classes.keys()])\n","\n","    self.audio_file = pd.concat([self.audio_file, self.label_df], axis=1)\n","\n","  def get_instrument_class_dict(self):\n","    return self.instrument_classes\n","\n","  def __len__(self):\n","    return len(self.audio_file.index)\n","\n","  def __getitem__(self, idx):\n","    # Allow for slicing\n","    if torch.is_tensor(idx):\n","      idx = idx.tolist()\n","\n","    if type(idx) is int:\n","      idx = [idx]\n","\n","    # Get the instruments types on hot encoded\n","    instrument_types = np.array(self.audio_file.iloc[idx, 3:]).astype(float)\n","    # Get the spectrograms as a numpy array from the npy files\n","    audios = np.array(self.audio_file.iloc[idx, 2])\n","\n","    spec_transforms = transforms.Compose([\n","        transforms.Normalize([-47.3835], [18.5056]),\n","    ])\n","    \n","    spec_array = []\n","    \n","    for file_idx, file_name in enumerate(audios):\n","      spec = np.load(file_name, allow_pickle=True).astype(float)\n","      spec = np.expand_dims(spec,-1)\n","      spec_array.append(spec)\n","    specs = np.stack(spec_array)\n","    specs = torch.from_numpy(specs)\n","    specs = specs.squeeze()\n","    specs = specs.double()\n","#     audios = np.expand_dims(audios, axis=-1)\n","#     audios = torch.from_numpy(audios)\n","\n","#     audios = np.stack(audios)\n","#     specs = np.squeeze(specs)\n","    # specs = np.repeat(specs[:,:,:,np.newaxis], 3, -1)\n","    # print(specs.shape)\n","\n","    instrument_types = torch.from_numpy(instrument_types)\n","    \n","    fs = 22050\n","    \n","    sample = {'specs': specs, \n","              'instrument(s)': instrument_types, \n","              'sample_key': self.audio_file.iloc[idx, 0].tolist(), \n","              'spec_type': self.spec_type,\n","              'fs': fs}\n","\n","    return sample"],"metadata":{"id":"-5p1-RVDiNci"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mel_spec_dataset = InstrumentDataset(csv_file='/content/drive/MyDrive/ECE6255_Project/openmic-2018-aggregated-labels.csv',\n","                                       json_file='/content/drive/MyDrive/ECE6255_Project/class-map.json',\n","                                       root_dir='/content/drive/MyDrive/ECE6255_Project',\n","                                       spec_type='mel_spectrogram')"],"metadata":{"id":"4bymsQfWp_1K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VGGish Dataset definition\n","vgg_dataset = VGGishInstrumentDataset(csv_file='/content/drive/MyDrive/ECE6255_Project/openmic-2018-aggregated-labels.csv',\n","                                       json_file='/content/drive/MyDrive/ECE6255_Project/class-map.json',\n","                                       root_dir='/content/drive/MyDrive/ECE6255_Project',\n","                                       spec_type='vgg')"],"metadata":{"id":"M3Hw0RVmibXT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 1\n","mel_train_size = int(0.8*len(mel_spec_dataset))\n","mel_test_size = len(mel_spec_dataset) - mel_train_size\n","\n","mel_train_set, mel_test_set = torch.utils.data.random_split(mel_spec_dataset, [mel_train_size, mel_test_size])\n","mel_val_size = int(0.25*mel_test_size)\n","mel_test_size = mel_test_size - mel_val_size\n","print(mel_train_size)\n","\n","mel_val_set, mel_test_set = torch.utils.data.random_split(mel_test_set, [mel_val_size, mel_test_size])\n","\n","mel_train_loader = DataLoader(mel_train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n","mel_val_loader = DataLoader(mel_val_set, batch_size=batch_size, shuffle=True, num_workers=2)\n","mel_test_loader = DataLoader(mel_test_set, batch_size=batch_size, shuffle=True, num_workers=2)"],"metadata":{"id":"YpbFt3zwqFif"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data splits\n","batch_size = 1\n","vgg_train_size = int(0.8*len(vgg_dataset))\n","vgg_test_size = len(vgg_dataset) - vgg_train_size\n","\n","vgg_train_set, vgg_test_set = torch.utils.data.random_split(vgg_dataset, [vgg_train_size, vgg_test_size])\n","vgg_val_size = int(0.25*vgg_test_size)\n","vgg_test_size = vgg_test_size - vgg_val_size\n","print(vgg_train_size)\n","\n","vgg_val_set, vgg_test_set = torch.utils.data.random_split(vgg_test_set, [vgg_val_size, vgg_test_size])\n","\n","vgg_train_loader = DataLoader(vgg_train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n","vgg_val_loader = DataLoader(vgg_val_set, batch_size=batch_size, shuffle=True, num_workers=2)\n","vgg_test_loader = DataLoader(vgg_test_set, batch_size=batch_size, shuffle=True, num_workers=2)"],"metadata":{"id":"ymmhr3FCjzkv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class MelSpecNetwork(nn.Module):\n","  def __init__(self):\n","    super(MelSpecNetwork, self).__init__()\n","\n","    self.resnet = models.resnet50(pretrained=True)\n","    default_in_ftrs = self.resnet.fc.in_features\n","    for param in self.resnet.parameters():\n","        param.requires_grad = False\n","    \n","    # Don't freeze last layer of resnet\n","    for param in self.resnet.layer4.parameters():\n","        param.requires_grad = True\n","\n","    # Replace fully connected layer to fit 20 instrument classes\n","    self.resnet.fc = nn.Linear(default_in_ftrs, 20)\n","    \n","    # self.sigmoid = nn.Sigmoid()\n","\n","  def forward(self, input):\n","    output = self.resnet(input)\n","    output = torch.sigmoid(output)\n","\n","    return output\n","\n","# VGGish Arch\n","class VggNetwork(nn.Module):\n","  def __init__(self):\n","    super(VggNetwork, self).__init__()\n","    \n","    urls = {\n","            'vggish': \"https://github.com/harritaylor/torchvggish/releases/download/v0.1/vggish-10086976.pth\",\n","            'pca': \"vggish_pca_params-970ea276.pth\"\n","        }\n","\n","    self.vggish_model = torch.hub.load('harritaylor/torchvggish', 'vggish', preprocess=False, postprocess=False)\n","#     for param in self.vggish_model.parameters():\n","#         param.requires_grad = False\n","    \n","#     # Don't freeze last layer of resnet\n","#     for param in self.resnet.layer4.parameters():\n","#         param.requires_grad = True\n","    # 10 seconds * 128 embedding size to 20 instrument classes\n","    self.classify = nn.Sequential(\n","            nn.Linear(10 * 128, 20),\n","        )       \n","\n","  def forward(self, input):\n","    bs, num_frames, _, _ = input.size()\n","    input = input.view(bs*num_frames, 1, input.size(2), input.size(3))\n","    vggish_logits = self.vggish_model(input) # [bs*num_frames, 128]\n","    vggish_logits = vggish_logits.reshape(bs, vggish_logits.size(1) * num_frames)\n","    \n","    output = self.classify(vggish_logits)\n","    output = torch.sigmoid(output)\n","\n","    return output"],"metadata":{"id":"N9wK7Z3Lnkv7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mel_spec_model = MelSpecNetwork()\n","mel_spec_model.eval()\n","mel_spec_model.double()\n","print(mel_spec_model)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","state_dict_name = 'mel_spectrogram_full.pt'\n","model_state_dict = torch.load('/content/drive/MyDrive/ECE6255_Project/models/' + state_dict_name, map_location=device)\n","mel_spec_model.load_state_dict(model_state_dict['model'])"],"metadata":{"id":"gBVKUuA5nnQH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vgg_model = VggNetwork()\n","vgg_model.eval()\n","vgg_model.double()\n","print(vgg_model)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","state_dict_name = 'vgg_full.pt'\n","model_state_dict = torch.load('/content/drive/MyDrive/ECE6255_Project/models/' + state_dict_name, map_location=device)\n","vgg_model.load_state_dict(model_state_dict['model'])"],"metadata":{"id":"XCVER-uThkEx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GradCAM class\n","class GradCAM:\n","  # init\n","  def __init__(self, model, target_layer):\n","    self.model = model.eval()\n","    self.target_layer = target_layer\n","    self.feature_maps = None\n","    self.gradients = None\n","    self.hooks = []\n","\n","    # extract feature maps\n","    def save_feature_maps(module, input, output):\n","      self.feature_maps = output.detach()\n","\n","    # extract gradients\n","    def save_gradients(module, grad_in, grad_out):\n","      self.gradients = grad_out[0].detach()\n","\n","    # register the functions as hooks for all the modules in the model\n","    for name, module in self.model.named_modules():\n","      if name == self.target_layer:\n","        self.hooks.append(module.register_forward_hook(save_feature_maps))\n","        self.hooks.append(module.register_backward_hook(save_gradients))\n","  \n","  # forward pass\n","  def forward(self, x):\n","    return self.model(x)\n","\n","  # backward pass\n","  def backward(self, index):\n","    one_hot = torch.zeros(self.feature_maps.shape[1:], dtype=torch.float32)\n","    one_hot[index] = 1.0\n","    one_hot = one_hot.unsqueeze(0).to(device=self.feature_maps.device)\n","    self.model.zero_grad()\n","    self.feature_maps.backward(gradient=one_hot, retain_graph=True)\n","\n","  # heatmap\n","  def generate_heatmap(self, index):\n","    weights = self.gradients.mean(dim=[2, 3], keepdim=True)\n","    heatmap = (weights * self.feature_maps).sum(dim=1, keepdim=True)\n","    heatmap = torch.relu(heatmap)\n","    heatmap /= torch.max(heatmap)\n","    return heatmap"],"metadata":{"id":"WiNGaYFgl-Pf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/drive/MyDrive/ECE6255_Project/class-map.json', 'r') as f:\n","    instrument_classes = json.load(f)\n","instrument_classes = list(instrument_classes.keys())\n","threshold = 0.5"],"metadata":{"id":"Rhkb02abYuhI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n","from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget, RawScoresOutputTarget, ClassifierOutputSoftmaxTarget\n","from pytorch_grad_cam.utils.image import show_cam_on_image\n","\n","from pytorch_grad_cam import DeepFeatureFactorization\n","from pytorch_grad_cam.utils.image import show_factorization_on_image\n","\n","# Get layer to analyze\n","layer_list = []\n","layer_to_analyze = mel_spec_model.resnet.layer4.children()\n","for layer in layer_to_analyze:\n","  layer_list.append(layer)\n","print(layer_list[2].conv3)\n","layer_target = layer_list[2].conv3\n","\n","# Get data\n","data = next(iter(mel_test_loader))\n","input_spec = data['specs']\n","print(data['sample_key'])\n","plt.imshow(data['specs'].squeeze().numpy()[0,:,:])\n","plt.show()\n","\n","# Transform spectrogram into an image with float values between 0 and 1\n","input_image = input_spec.squeeze().detach().numpy().transpose(1,2,0)\n","input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n","\n","# Create GradCAM object\n","gc = GradCAM(model=mel_spec_model, target_layers=[layer_target])\n","\n","# Create Deep Feature Factorization\n","dff = DeepFeatureFactorization(model=mel_spec_model, target_layer=layer_target, computation_on_concepts=mel_spec_model.resnet.fc)\n","\n","# Get true label indices\n","label_indices = torch.argwhere(data['instrument(s)'].squeeze(0))\n","label_indices_list = torch.argwhere(data['instrument(s)'].squeeze(0))\n","label_indices_list = label_indices_list[:, 1].detach().numpy()\n","print([instrument_classes[i] for i in label_indices_list])\n","\n","# Get prediction label indices\n","pred_indices_list = np.array((mel_spec_model(input_spec).squeeze(0).detach().numpy() > threshold).astype(int).nonzero()).squeeze(0)\n","print([instrument_classes[i] for i in pred_indices_list])\n","\n","input_spec.requires_grad_()\n","input_spec.retain_grad()\n","\n","class ClassifyTarget:\n","    def __init__(self, category):\n","        self.category = category\n","\n","    def __call__(self, model_output):\n","        if len(model_output.shape) == 1:\n","            print(model_output[self.category])\n","            return model_output[self.category]\n","        return model_output[:, self.category]\n","\n","plt.rcParams['figure.figsize'] = (10, 5)\n","\n","for label in pred_indices_list:\n","  classifier_targets = ClassifyTarget(label)\n","  result = gc(input_tensor=input_spec, targets=[classifier_targets])\n","  result = result[0, :]\n","  print(\"Why label: \", instrument_classes[label])\n","  visualization = show_cam_on_image(input_image, result, use_rgb=True)\n","  plt.imshow(visualization)\n","  plt.show()\n","  \n","plt.rcParams['figure.figsize'] = (20, 10)\n","top_k = 2\n","mel_spec_model.float()\n","concepts, batch_explanations, concept_scores = dff(input_spec.float(), 4)\n","mel_spec_model.double()\n","concept_scores = torch.sigmoid(torch.from_numpy(concept_scores)).numpy()\n","concept_categories = np.argsort(concept_scores, axis=1)[:, ::-1][:, :top_k]\n","concept_labels_topk = []\n","for concept_index in range(concept_categories.shape[0]):\n","    categories = concept_categories[concept_index, :]    \n","    concept_labels = []\n","    for category in categories:\n","        score = concept_scores[concept_index, category]\n","        label = f\"{instrument_classes[category].split(',')[0]}:{score:.2f}\"\n","        concept_labels.append(label)\n","    concept_labels_topk.append(\"\\n\".join(concept_labels))\n","visualization = show_factorization_on_image(input_image, \n","                                            batch_explanations[0],\n","                                            image_weight=0.3,\n","                                            concept_labels=concept_labels_topk)\n","plt.imshow(visualization)\n","plt.show()\n","plt.rcParams['figure.figsize'] = (10, 5)\n"],"metadata":{"id":"msos888jx7vg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n","from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget, RawScoresOutputTarget, ClassifierOutputSoftmaxTarget\n","from pytorch_grad_cam.utils.image import show_cam_on_image\n","\n","plt.rcParams['figure.figsize'] = (20, 10)\n","vgg_model.double()\n","\n","# Get layer to analyze\n","vgg_layer_list = []\n","vgg_layer_to_analyze = vgg_model.vggish_model.features.children()\n","for vgg_layer in vgg_layer_to_analyze:\n","  vgg_layer_list.append(vgg_layer)\n","print(vgg_layer_list[13])\n","vgg_layer_target = vgg_layer_list[13]\n","\n","# Get data\n","data = next(iter(vgg_test_loader))\n","input_spec = data['specs']\n","print(data['sample_key'])\n","\n","# Transform spectrogram into an image with float values between 0 and 1\n","input_image = input_spec.squeeze().detach().numpy()\n","input_image = input_image.reshape(input_image.shape[0] * input_image.shape[1], input_image.shape[2]).T\n","plt.imshow(input_image)\n","plt.show()\n","input_image = np.expand_dims(input_image, -1)\n","input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n","\n","# Create GradCAM object\n","gc = GradCAM(model=vgg_model, target_layers=[vgg_layer_target])\n","\n","# Create Deep Feature Factorization\n","dff = DeepFeatureFactorization(model=vgg_model, target_layer=vgg_model.vggish_model.embeddings,\n","                               computation_on_concepts=vgg_model.classify)\n","\n","# Get true label indices\n","label_indices = torch.argwhere(data['instrument(s)'].squeeze(0))\n","label_indices_list = torch.argwhere(data['instrument(s)'].squeeze(0))\n","label_indices_list = label_indices_list[:,1].detach().numpy()\n","print([instrument_classes[i] for i in label_indices_list])\n","\n","# Get prediction label indices\n","pred_indices_list = np.array((vgg_model(input_spec).squeeze(0).detach().numpy() > threshold).astype(int).nonzero()).squeeze(0)\n","print([instrument_classes[i] for i in pred_indices_list])\n","\n","input_spec.requires_grad_()\n","input_spec.retain_grad()\n","\n","class ClassifyTarget:\n","    def __init__(self, category):\n","        self.category = category\n","\n","    def __call__(self, model_output):\n","        if len(model_output.shape) == 1:\n","            return model_output[self.category]\n","        return model_output[:, self.category]\n","\n","for label in pred_indices_list:\n","  classifier_targets = ClassifyTarget(label)\n","  result = gc(input_tensor=input_spec, targets=[classifier_targets])\n","  result = result.reshape(result.shape[0] * result.shape[1], result.shape[2]).T\n","  print(\"Why label: \", instrument_classes[label])\n","  visualization = show_cam_on_image(input_image, result, use_rgb=True)\n","  plt.imshow(visualization)\n","  plt.show()\n","\n","# top_k = 2\n","# vgg_model.float()\n","# print(input_spec.shape)\n","# concepts, batch_explanations, concept_scores = dff(input_spec.float(), 5)\n","# vgg_model.double()\n","# concept_scores = torch.sigmoid(torch.from_numpy(concept_scores)).numpy()\n","# concept_categories = np.argsort(concept_scores, axis=1)[:, ::-1][:, :top_k]\n","# concept_labels_topk = []\n","# for concept_index in range(concept_categories.shape[0]):\n","#     categories = concept_categories[concept_index, :]    \n","#     concept_labels = []\n","#     for category in categories:\n","#         score = concept_scores[concept_index, category]\n","#         label = f\"{instrument_classes[category].split(',')[0]}:{score:.2f}\"\n","#         concept_labels.append(label)\n","#     concept_labels_topk.append(\"\\n\".join(concept_labels))\n","# print(len(batch_explanations))\n","# print(batch_explanations[0].shape)\n","# batch_explanations = np.stack(batch_explanations).transpose(0,1,3,2).reshape(5, 64, 960)\n","# print(batch_explanations.shape)\n","# print(batch_explanations)\n","# visualization = show_factorization_on_image(input_image, \n","#                                             batch_explanations,\n","#                                             image_weight=0.3,\n","#                                             concept_labels=concept_labels_topk)\n","# plt.imshow(visualization)\n","# plt.show()\n","# plt.rcParams['figure.figsize'] = (10, 5)"],"metadata":{"id":"B9HFq7BrpwjX"},"execution_count":null,"outputs":[]}]}