{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install resampy\n# %pip install torchvggish","metadata":{"id":"NM5Hu_ReGUXr","execution":{"iopub.status.busy":"2023-04-18T02:18:24.213857Z","iopub.execute_input":"2023-04-18T02:18:24.214273Z","iopub.status.idle":"2023-04-18T02:18:36.405917Z","shell.execute_reply.started":"2023-04-18T02:18:24.214235Z","shell.execute_reply":"2023-04-18T02:18:36.404611Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting resampy\n  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numba>=0.53 in /opt/conda/lib/python3.7/site-packages (from resampy) (0.56.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from resampy) (1.21.6)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from numba>=0.53->resampy) (59.8.0)\nRequirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /opt/conda/lib/python3.7/site-packages (from numba>=0.53->resampy) (0.39.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from numba>=0.53->resampy) (4.11.4)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->numba>=0.53->resampy) (4.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->numba>=0.53->resampy) (3.11.0)\nInstalling collected packages: resampy\nSuccessfully installed resampy-0.4.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport json\nimport os\n\nimport torch\nimport torchvision\nfrom torchvision import transforms\nfrom torchvision import models\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm, tqdm_notebook\nfrom torch.autograd import Variable\nfrom fastprogress.fastprogress import format_time, master_bar, progress_bar\nfrom sklearn.metrics import f1_score, jaccard_score\n\nfrom matplotlib import pyplot as plt\n\nfrom sklearn import preprocessing\nimport soundfile as sf\n\nimport librosa\n# from torchvggish import vggish, vggish_input","metadata":{"execution":{"iopub.status.busy":"2023-04-18T02:18:36.409333Z","iopub.execute_input":"2023-04-18T02:18:36.410695Z","iopub.status.idle":"2023-04-18T02:18:39.731779Z","shell.execute_reply.started":"2023-04-18T02:18:36.410642Z","shell.execute_reply":"2023-04-18T02:18:39.730245Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"nnQ6Q_0aIWRu","outputId":"0fb33a3f-a4f9-48ec-fbbb-bd99060687e8","execution":{"iopub.status.busy":"2023-04-18T02:01:54.714135Z","iopub.execute_input":"2023-04-18T02:01:54.714515Z","iopub.status.idle":"2023-04-18T02:01:54.724428Z","shell.execute_reply.started":"2023-04-18T02:01:54.714477Z","shell.execute_reply":"2023-04-18T02:01:54.723368Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"tqdm.pandas()","metadata":{"id":"0AfgIfvuAIHJ","execution":{"iopub.status.busy":"2023-04-18T02:01:54.729846Z","iopub.execute_input":"2023-04-18T02:01:54.730183Z","iopub.status.idle":"2023-04-18T02:01:54.736312Z","shell.execute_reply.started":"2023-04-18T02:01:54.730152Z","shell.execute_reply":"2023-04-18T02:01:54.735232Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(0)\ntorch.cuda.manual_seed(0)","metadata":{"id":"gDnsX0Bour55","execution":{"iopub.status.busy":"2023-04-18T02:01:54.737933Z","iopub.execute_input":"2023-04-18T02:01:54.738963Z","iopub.status.idle":"2023-04-18T02:01:54.745122Z","shell.execute_reply.started":"2023-04-18T02:01:54.738918Z","shell.execute_reply":"2023-04-18T02:01:54.744037Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"class InstrumentDataset(Dataset):\n  def __init__(self, csv_file, json_file, root_dir, spec_type):\n    self.audio_frame = pd.read_csv(csv_file)\n    with open(json_file, 'r') as f:\n      self.instrument_classes = json.load(f)\n    \n    self.json_file = json_file\n    self.csv_file = csv_file\n    self.root_dir = root_dir\n\n    # Add chroma_cqt if desired\n    if spec_type not in [\"mel_spectrogram\", \"chroma_stft\", \"cqt\"]:\n      raise Exception(\"Not valid spectrogram type\")\n    else:\n      self.spec_type = spec_type\n\n    # List of unique sample_keys (aka audio file names)\n    self.unique_audio_files = self.audio_frame.sample_key.unique()\n\n    # Dataframe specifying the sample_key of the audio file and the instrument labels\n    self.audio_file_labels = self.audio_frame.groupby('sample_key')['instrument'].apply(list).reset_index(name='labels')\n\n#     self.audio_file = self.audio_file_labels.iloc[:5000,:].copy()\n    self.audio_file = self.audio_file_labels.copy()\n\n    num_data = self.audio_file.shape\n\n    os_join = np.vectorize(os.path.join)\n\n    self.audio_file[spec_type] = np.full(num_data[0], [0])\n    self.audio_file[spec_type] = os_join(np.full(num_data[0], self.root_dir), np.full(num_data[0], spec_type), np.full(num_data[0], spec_type),\n                                          self.audio_file.sample_key.str[:3], \n                                          self.audio_file.sample_key.str[:] + np.full(num_data[0], '_' + spec_type + '.npy'))\n\n    # Matrix of instrument labels ordered by audio file number (increasing sample_key value)\n    # self.label_matrix = self.audio_file_labels.labels.tolist()\n    self.label_matrix = self.audio_file.labels.tolist()\n\n    binarizer = preprocessing.MultiLabelBinarizer()\n    \n    self.binary_label_matrix = binarizer.fit_transform(self.label_matrix)\n    self.label_df = pd.DataFrame(self.binary_label_matrix,columns=[instrument for instrument in self.instrument_classes.keys()])\n\n    self.audio_file = pd.concat([self.audio_file, self.label_df], axis=1)\n\n  def get_instrument_class_dict(self):\n    return self.instrument_classes\n\n  def __len__(self):\n    return len(self.audio_file.index)\n\n  def __getitem__(self, idx):\n    # Allow for slicing\n    if torch.is_tensor(idx):\n      idx = idx.tolist()\n\n    if type(idx) is int:\n      idx = [idx]\n\n    # Get the instruments types on hot encoded\n    instrument_types = np.array(self.audio_file.iloc[idx, 3:]).astype(float)\n    # Get the spectrograms as a numpy array from the npy files\n    specs = np.array(self.audio_file.iloc[idx, 2])\n\n    spec_transforms = transforms.Compose([\n        transforms.Normalize([-47.3835], [18.5056]),\n    ])\n    \n    spec_array = []\n    \n    for file_idx, file_name in enumerate(specs):\n      # 3 channel expanded\n      spec = np.load(file_name).astype(float)\n#       plt.imshow(spec)\n#       plt.show()\n      spec = np.expand_dims(spec,-1)\n#       spec = np.repeat(spec, 3, -1)\n      # 1 channel\n      # spec = np.load(file_name).astype(float)\n      # spec = 2*(spec - np.min(spec))/np.ptp(spec) - 1\n#       spec = spec_transforms(spec)\n#       print((spec - np.min(spec))/np.ptp(spec))\n#       plt.imshow((spec - np.min(spec))/np.ptp(spec))\n#       plt.show()\n      spec_array.append(spec)\n    specs = np.asarray(spec_array)\n    specs = specs.transpose((0,-1,1,2))\n    specs = torch.from_numpy(specs)\n    specs = spec_transforms(specs)\n    specs = specs.squeeze(0)\n\n    specs = np.stack(specs)\n#     print(specs.shape)\n\n    instrument_types = torch.from_numpy(instrument_types)\n\n    \n    sample = {'specs': specs, \n              'instrument(s)': instrument_types, \n              'sample_key': self.audio_file.iloc[idx, 0].tolist(), \n              'spec_type': self.spec_type}\n\n    return sample","metadata":{"id":"tn-W1eCIWLDT","execution":{"iopub.status.busy":"2023-04-18T02:30:29.677042Z","iopub.execute_input":"2023-04-18T02:30:29.678174Z","iopub.status.idle":"2023-04-18T02:30:29.700221Z","shell.execute_reply.started":"2023-04-18T02:30:29.678104Z","shell.execute_reply":"2023-04-18T02:30:29.699102Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"class VGGishInstrumentDataset(Dataset):\n  def __init__(self, csv_file, json_file, root_dir, spec_type):\n    self.audio_frame = pd.read_csv(csv_file)\n    with open(json_file, 'r') as f:\n      self.instrument_classes = json.load(f)\n    \n    self.json_file = json_file\n    self.csv_file = csv_file\n    self.root_dir = root_dir\n\n    # Add chroma_cqt if desired\n    if spec_type not in [\"vgg\", \"audio\", \"mel_spectrogram\"]:\n      raise Exception(\"Not valid spectrogram type\")\n    else:\n      self.spec_type = spec_type\n\n    # List of unique sample_keys (aka audio file names)\n    self.unique_audio_files = self.audio_frame.sample_key.unique()\n\n    # Dataframe specifying the sample_key of the audio file and the instrument labels\n    self.audio_file_labels = self.audio_frame.groupby('sample_key')['instrument'].apply(list).reset_index(name='labels')\n\n#     self.audio_file = self.audio_file_labels.iloc[:500,:].copy()\n    self.audio_file = self.audio_file_labels.copy()\n\n    num_data = self.audio_file.shape\n\n    os_join = np.vectorize(os.path.join)\n\n    self.audio_file[spec_type] = np.full(num_data[0], [0])\n#     self.audio_file[spec_type] = os_join(np.full(num_data[0], self.root_dir), np.full(num_data[0], 'audio'), np.full(num_data[0], 'audio'),\n#                                           self.audio_file.sample_key.str[:3], \n#                                           self.audio_file.sample_key.str[:] + np.full(num_data[0], '.ogg'))\n    self.audio_file[spec_type] = os_join(np.full(num_data[0], self.root_dir), np.full(num_data[0], self.spec_type), np.full(num_data[0], self.spec_type),\n                                          self.audio_file.sample_key.str[:3], \n                                          self.audio_file.sample_key.str[:] + np.full(num_data[0], '_' + spec_type + '.npy'))\n\n    # Matrix of instrument labels ordered by audio file number (increasing sample_key value)\n    # self.label_matrix = self.audio_file_labels.labels.tolist()\n    self.label_matrix = self.audio_file.labels.tolist()\n\n    binarizer = preprocessing.MultiLabelBinarizer()\n    \n    self.binary_label_matrix = binarizer.fit_transform(self.label_matrix)\n    self.label_df = pd.DataFrame(self.binary_label_matrix,columns=[instrument for instrument in self.instrument_classes.keys()])\n\n    self.audio_file = pd.concat([self.audio_file, self.label_df], axis=1)\n\n  def get_instrument_class_dict(self):\n    return self.instrument_classes\n\n  def __len__(self):\n    return len(self.audio_file.index)\n\n  def __getitem__(self, idx):\n    # Allow for slicing\n    if torch.is_tensor(idx):\n      idx = idx.tolist()\n\n    if type(idx) is int:\n      idx = [idx]\n\n    # Get the instruments types on hot encoded\n    instrument_types = np.array(self.audio_file.iloc[idx, 3:]).astype(float)\n    # Get the spectrograms as a numpy array from the npy files\n    audios = np.array(self.audio_file.iloc[idx, 2])\n\n    spec_transforms = transforms.Compose([\n        transforms.Normalize([-47.3835], [18.5056]),\n    ])\n    \n    spec_array = []\n    \n    for file_idx, file_name in enumerate(audios):\n      spec = np.load(file_name, allow_pickle=True).astype(float)\n      spec = np.expand_dims(spec,-1)\n      spec_array.append(spec)\n    specs = np.stack(spec_array)\n    specs = torch.from_numpy(specs)\n    specs = specs.squeeze()\n    specs = specs.double()\n\n#     audios = np.expand_dims(audios, axis=-1)\n#     audios = torch.from_numpy(audios)\n\n#     audios = np.stack(audios)\n#     specs = np.squeeze(specs)\n    # specs = np.repeat(specs[:,:,:,np.newaxis], 3, -1)\n    # print(specs.shape)\n\n    instrument_types = torch.from_numpy(instrument_types)\n    \n    fs = 22050\n    \n    sample = {'specs': specs, \n              'instrument(s)': instrument_types, \n              'sample_key': self.audio_file.iloc[idx, 0].tolist(), \n              'spec_type': self.spec_type,\n              'fs': fs}\n\n    return sample","metadata":{"execution":{"iopub.status.busy":"2023-04-18T02:23:05.472143Z","iopub.execute_input":"2023-04-18T02:23:05.472550Z","iopub.status.idle":"2023-04-18T02:23:05.706601Z","shell.execute_reply.started":"2023-04-18T02:23:05.472495Z","shell.execute_reply":"2023-04-18T02:23:05.705234Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Dataset definition\nmel_spec_dataset = InstrumentDataset(csv_file='/kaggle/input/spec-data/openmic-2018-aggregated-labels.csv',\n                                       json_file='/kaggle/input/spec-data/class-map.json',\n                                       root_dir='/kaggle/input/spec-data',\n                                       spec_type='mel_spectrogram')","metadata":{"id":"0RV3NeaEILN0","execution":{"iopub.status.busy":"2023-04-18T02:30:33.393973Z","iopub.execute_input":"2023-04-18T02:30:33.394344Z","iopub.status.idle":"2023-04-18T02:30:33.855068Z","shell.execute_reply.started":"2023-04-18T02:30:33.394311Z","shell.execute_reply":"2023-04-18T02:30:33.854022Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"# VGGish Dataset definition\nvgg_dataset = VGGishInstrumentDataset(csv_file='/kaggle/input/spec-data/openmic-2018-aggregated-labels.csv',\n                                       json_file='/kaggle/input/spec-data/class-map.json',\n                                       root_dir='/kaggle/input/spec-data',\n                                       spec_type='vgg')","metadata":{"execution":{"iopub.status.busy":"2023-04-18T02:18:40.803296Z","iopub.execute_input":"2023-04-18T02:18:40.803750Z","iopub.status.idle":"2023-04-18T02:18:41.437765Z","shell.execute_reply.started":"2023-04-18T02:18:40.803712Z","shell.execute_reply":"2023-04-18T02:18:41.436468Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def mean_std(dataloader):\n  # running_sum = 0\n  # for i, sample in enumerate(progress_bar(dataloader)):\n  #   specs = sample['specs']\n\n  #   specs = specs[:,0,:,:]\n\n  #   running_sum += specs.sum()\n  running_sum = 0\n  running_squared_sum = 0\n  running_n = 0\n  for batch in dataloader:\n    batch = batch['specs']\n    running_sum += batch.sum()\n    running_squared_sum += batch.square().sum()\n    running_n += batch.shape[0] * batch.shape[1] * batch.shape[2]\n    print(running_sum)\n    print(running_squared_sum)\n  print(running_n)\n  mean = running_sum / running_n\n  std = np.sqrt(running_squared_sum / running_n - np.square(running_sum / running_n))\n\n  return mean, std","metadata":{"id":"C2eXC8y3tJtR","execution":{"iopub.status.busy":"2023-04-18T02:01:55.770450Z","iopub.execute_input":"2023-04-18T02:01:55.770760Z","iopub.status.idle":"2023-04-18T02:01:55.777533Z","shell.execute_reply.started":"2023-04-18T02:01:55.770720Z","shell.execute_reply":"2023-04-18T02:01:55.776296Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"# loader = DataLoader(mel_spec_dataset, batch_size=1000, num_workers=0)\n# mel_mean, mel_std = mean_std(loader)\n# print(mel_mean)\n# print(mel_std)","metadata":{"id":"SHykXDfmuNoX","outputId":"f7369aa2-a385-40a5-8874-54d8f934d4cb","execution":{"iopub.status.busy":"2023-04-18T02:01:55.779070Z","iopub.execute_input":"2023-04-18T02:01:55.779547Z","iopub.status.idle":"2023-04-18T02:01:55.787479Z","shell.execute_reply.started":"2023-04-18T02:01:55.779511Z","shell.execute_reply":"2023-04-18T02:01:55.786375Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"data = mel_spec_dataset[5:7]['specs']\n\nprint(data)\nprint(data.shape)\n# plt.imshow(data.transpose(1,2,0))\n# plt.show()\n","metadata":{"id":"9T0aCLr_9au6","outputId":"5a4428c9-be58-4c1f-ffb9-f69d701d7e5f","execution":{"iopub.status.busy":"2023-04-18T02:30:37.953427Z","iopub.execute_input":"2023-04-18T02:30:37.954344Z","iopub.status.idle":"2023-04-18T02:30:37.971140Z","shell.execute_reply.started":"2023-04-18T02:30:37.954290Z","shell.execute_reply":"2023-04-18T02:30:37.970041Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"[[[[-0.36767249  0.447959    0.61176076 ...  0.81777948  0.80595874\n     0.92247752]\n   [-0.34065364  0.62695887  0.78907196 ...  1.01451047  1.07192553\n     1.2770999 ]\n   [-0.04682366  0.95625108  1.4307696  ...  1.85251694  1.89726691\n     1.7921467 ]\n   ...\n   [-1.76252053 -1.39438873 -1.00261542 ... -0.21400279 -0.53991765\n    -0.86752118]\n   [-1.76252053 -1.76252053 -1.50584148 ... -1.09042668 -1.27280391\n    -1.5497471 ]\n   [-1.76252053 -1.76252053 -1.76252053 ... -1.76252053 -1.76252053\n    -1.76252053]]]\n\n\n [[[-0.35247439  0.8397323   1.00691142 ...  0.3398836   0.21492143\n     0.81777948]\n   [-0.3744272   1.11751983  1.53293463 ...  1.58021761  1.51858086\n     1.60048175]\n   [ 0.01228007  1.36406682  1.78074812 ...  1.99478806  1.95890366\n     1.83056413]\n   ...\n   [-1.76252053 -1.76252053 -1.76252053 ... -1.76252053 -1.76252053\n    -1.76252053]\n   [-1.76252053 -1.76252053 -1.76252053 ... -1.76252053 -1.76252053\n    -1.76252053]\n   [-1.76252053 -1.76252053 -1.76252053 ... -1.76252053 -1.76252053\n    -1.76252053]]]]\n(2, 1, 128, 431)\n","output_type":"stream"}]},{"cell_type":"code","source":"vgg_data = vgg_dataset[5:7]['specs']\nprint(vgg_data)\nfs = vgg_dataset[5:6]['fs']\nprint(fs)\n\nprint(vgg_data.shape)\n# plt.imshow(data.transpose(1,2,0))\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-18T02:18:52.377695Z","iopub.execute_input":"2023-04-18T02:18:52.378226Z","iopub.status.idle":"2023-04-18T02:18:52.522260Z","shell.execute_reply.started":"2023-04-18T02:18:52.378180Z","shell.execute_reply":"2023-04-18T02:18:52.520975Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"tensor([[[[-4.6052e+00, -4.6052e+00, -4.6052e+00,  ..., -4.6052e+00,\n           -4.6051e+00, -4.6051e+00],\n          [-3.8980e+00, -3.8998e+00, -3.9142e+00,  ..., -3.7212e+00,\n           -3.9750e+00, -4.1558e+00],\n          [-6.1428e-02, -8.2499e-02,  7.0901e-02,  ..., -9.1753e-01,\n           -1.0298e+00, -1.0156e+00],\n          ...,\n          [ 9.7568e-01,  7.9385e-01,  8.2498e-01,  ..., -5.8668e-01,\n           -7.1539e-01, -1.4541e+00],\n          [ 3.8418e-01,  4.2612e-01,  2.7493e-01,  ..., -7.9187e-01,\n           -7.2015e-01, -9.8585e-01],\n          [ 9.0007e-01,  7.5159e-01,  8.8080e-01,  ..., -1.0713e+00,\n           -1.2385e+00, -1.3461e+00]],\n\n         [[ 3.3946e-02,  4.4372e-01,  9.8933e-01,  ...,  5.0405e-01,\n            6.9974e-02, -4.8244e-02],\n          [ 9.2487e-01,  6.3326e-01,  6.7975e-01,  ..., -3.3901e-02,\n            7.5498e-02, -9.6056e-02],\n          [ 8.6667e-01,  8.8299e-01,  6.0556e-01,  ...,  4.4148e-01,\n           -4.1301e-01, -3.9728e-01],\n          ...,\n          [ 1.8700e+00,  1.9277e+00,  1.3499e+00,  ..., -5.1911e-02,\n           -4.8820e-01, -2.5708e-01],\n          [ 1.3700e+00,  1.6946e+00,  7.2824e-01,  ..., -4.4647e-02,\n           -1.3436e-01, -5.8086e-02],\n          [ 1.7547e+00,  1.4723e+00,  6.6883e-01,  ...,  8.3926e-02,\n           -1.0608e-01, -3.9150e-01]],\n\n         [[ 1.4928e+00,  8.8773e-01,  2.1581e-01,  ..., -1.1980e-03,\n           -3.8093e-01, -3.0075e-01],\n          [-2.9363e-01,  6.0091e-01,  5.4300e-01,  ...,  6.0159e-02,\n           -6.0213e-01, -4.0111e-01],\n          [ 1.0986e+00,  5.3404e-01,  2.1556e-01,  ..., -6.0442e-02,\n           -1.7090e-01, -6.4790e-02],\n          ...,\n          [ 8.5723e-01,  7.4407e-02,  2.3152e-01,  ...,  7.1791e-02,\n           -1.6408e-01, -1.0082e+00],\n          [ 1.1077e+00,  8.3689e-01,  1.1200e+00,  ..., -1.3481e-01,\n           -9.2647e-02, -9.7420e-01],\n          [ 9.6397e-01,  1.1255e+00,  1.0555e+00,  ...,  3.0785e-01,\n            4.6510e-02, -9.2492e-01]],\n\n         ...,\n\n         [[ 5.0760e-01, -2.6330e-02,  2.3327e-01,  ...,  1.0025e+00,\n            1.1099e+00,  6.6401e-01],\n          [ 7.2054e-01,  5.0699e-01,  6.8776e-02,  ...,  8.5020e-01,\n            4.6103e-01,  6.4175e-02],\n          [ 8.2877e-02, -7.3887e-01, -7.7725e-02,  ...,  4.6207e-02,\n            2.3264e-01, -7.3320e-02],\n          ...,\n          [ 2.3041e+00,  1.4102e+00,  1.7291e+00,  ...,  5.3752e-01,\n            8.4215e-01,  4.7489e-01],\n          [ 3.0165e+00,  2.7050e+00,  1.9993e+00,  ...,  1.8659e+00,\n            1.5524e+00,  6.4377e-01],\n          [ 2.6306e+00,  2.7246e+00,  2.1399e+00,  ...,  6.9168e-01,\n            6.1650e-01,  1.5903e-01]],\n\n         [[ 2.2521e+00,  1.9719e+00,  1.1554e+00,  ...,  6.6626e-01,\n            5.5771e-01, -2.3933e-01],\n          [ 1.5109e+00,  1.6199e+00,  1.3380e+00,  ...,  5.8571e-01,\n            4.9870e-01, -3.4414e-01],\n          [ 1.7268e+00,  1.9141e+00,  1.5055e+00,  ...,  2.2394e-01,\n            4.2598e-01, -4.5769e-01],\n          ...,\n          [ 1.1301e+00,  9.5440e-01,  2.9417e-01,  ...,  7.3415e-02,\n            5.6800e-01,  5.3631e-02],\n          [ 1.6892e+00,  1.1389e+00,  1.1417e-02,  ..., -1.2953e-02,\n            7.5891e-01,  5.3745e-01],\n          [ 6.2803e-01,  2.3374e+00,  2.4126e+00,  ...,  1.3889e+00,\n            1.2176e+00,  7.6098e-01]],\n\n         [[ 2.3995e+00,  2.6809e+00,  2.3782e+00,  ...,  2.0904e+00,\n            2.1047e+00,  1.5749e+00],\n          [ 1.9399e+00,  2.2606e+00,  1.9870e+00,  ...,  1.7204e+00,\n            1.8851e+00,  1.5803e+00],\n          [ 6.0472e-01,  1.5726e+00,  1.8205e+00,  ...,  1.5070e+00,\n            1.6573e+00,  1.4409e+00],\n          ...,\n          [ 1.8439e+00,  1.6268e+00,  8.4767e-01,  ..., -1.0094e+00,\n           -3.6560e-01, -8.9800e-01],\n          [ 1.9023e+00,  1.3290e+00, -5.5254e-01,  ..., -1.5618e+00,\n           -1.0187e+00, -1.2845e+00],\n          [ 1.5815e+00,  1.1099e+00,  6.8474e-01,  ..., -1.8083e+00,\n           -1.2833e+00, -1.3250e+00]]],\n\n\n        [[[-3.8797e+00, -4.1283e+00, -4.1448e+00,  ..., -4.5826e+00,\n           -4.5760e+00, -4.5912e+00],\n          [-3.3030e+00, -2.4404e+00, -1.9180e+00,  ..., -4.4753e+00,\n           -4.3984e+00, -4.5153e+00],\n          [ 1.9138e+00,  1.8351e+00,  1.5017e+00,  ..., -4.2702e+00,\n           -4.1616e+00, -4.4794e+00],\n          ...,\n          [ 1.9565e+00,  2.7741e+00,  2.8306e+00,  ..., -3.8943e+00,\n           -3.9781e+00, -3.9531e+00],\n          [ 1.7908e+00,  2.2666e+00,  2.1277e+00,  ..., -3.8305e+00,\n           -3.8227e+00, -4.1111e+00],\n          [-5.3162e-02,  1.3822e+00,  6.6911e-01,  ..., -4.2112e+00,\n           -4.1434e+00, -4.0330e+00]],\n\n         [[ 1.1077e+00,  2.1098e+00,  2.0368e+00,  ..., -4.2621e+00,\n           -4.1993e+00, -4.0158e+00],\n          [ 1.4542e+00,  2.2392e+00,  2.3590e+00,  ..., -3.9643e+00,\n           -4.0654e+00, -4.2465e+00],\n          [ 1.0156e+00,  1.9964e+00,  2.1792e+00,  ..., -3.6692e+00,\n           -3.7635e+00, -4.1267e+00],\n          ...,\n          [ 1.2127e+00,  2.2018e+00,  2.3317e+00,  ..., -4.1399e+00,\n           -4.0602e+00, -4.1338e+00],\n          [ 1.1562e+00,  2.0314e+00,  2.0107e+00,  ..., -3.8566e+00,\n           -3.8958e+00, -4.0485e+00],\n          [ 1.9678e+00,  1.4509e+00,  1.6435e+00,  ..., -3.8343e+00,\n           -3.9159e+00, -3.8182e+00]],\n\n         [[ 1.9084e+00,  2.4296e+00,  2.5420e+00,  ..., -3.9985e+00,\n           -3.7999e+00, -3.9500e+00],\n          [ 2.2950e+00,  2.8516e+00,  2.8869e+00,  ..., -4.0945e+00,\n           -3.7180e+00, -4.1263e+00],\n          [ 1.6984e+00,  2.6254e+00,  2.8702e+00,  ..., -3.9609e+00,\n           -3.8196e+00, -3.8574e+00],\n          ...,\n          [-6.2958e-01,  9.5528e-01,  1.1423e+00,  ..., -4.2000e+00,\n           -4.2486e+00, -4.2951e+00],\n          [ 6.2733e-01,  2.0336e+00,  2.1211e+00,  ..., -4.2561e+00,\n           -4.3554e+00, -4.3088e+00],\n          [ 1.3637e+00,  2.4213e+00,  2.4945e+00,  ..., -4.2998e+00,\n           -4.2967e+00, -4.3859e+00]],\n\n         ...,\n\n         [[-6.1690e-01,  4.7775e-01,  7.3007e-01,  ..., -3.8928e+00,\n           -4.0106e+00, -4.1920e+00],\n          [ 2.3232e-01,  1.0574e+00,  1.1442e+00,  ..., -4.1282e+00,\n           -4.1584e+00, -4.2537e+00],\n          [-4.5768e-01,  6.3780e-01,  9.5038e-01,  ..., -4.1340e+00,\n           -4.0876e+00, -4.4060e+00],\n          ...,\n          [-6.7448e-01,  7.2793e-01,  8.7287e-01,  ..., -4.0027e+00,\n           -4.0425e+00, -4.0490e+00],\n          [-5.8068e-01,  2.2210e-02,  5.1246e-01,  ..., -3.8872e+00,\n           -3.9769e+00, -3.9808e+00],\n          [-7.7375e-01, -1.8082e-01, -3.8147e-01,  ..., -3.7830e+00,\n           -3.9106e+00, -3.8957e+00]],\n\n         [[ 1.2436e-01,  1.7969e-01,  4.9449e-01,  ..., -3.7325e+00,\n           -4.0205e+00, -4.1448e+00],\n          [ 3.2722e-01,  8.6703e-01,  8.9610e-01,  ..., -4.0231e+00,\n           -4.0509e+00, -4.1336e+00],\n          [ 1.9770e-01,  8.5186e-01,  9.1931e-01,  ..., -4.0969e+00,\n           -3.8728e+00, -4.1004e+00],\n          ...,\n          [ 1.5611e-01,  2.8698e-01,  1.3469e-01,  ..., -3.8454e+00,\n           -3.9951e+00, -3.9375e+00],\n          [-3.1617e-01,  4.2854e-01,  6.3683e-01,  ..., -3.9623e+00,\n           -4.1151e+00, -4.2673e+00],\n          [ 2.3520e-01,  8.5796e-01,  5.6823e-01,  ..., -4.1333e+00,\n           -4.1834e+00, -4.2884e+00]],\n\n         [[-2.1475e-01,  4.8933e-01,  1.8822e-01,  ..., -3.8265e+00,\n           -4.0892e+00, -4.0682e+00],\n          [-1.0266e+00, -5.7418e-01, -4.1486e-01,  ..., -4.0030e+00,\n           -4.0973e+00, -4.1348e+00],\n          [ 1.2460e-02, -3.1410e-02, -4.0167e-01,  ..., -3.9305e+00,\n           -4.0339e+00, -4.0986e+00],\n          ...,\n          [ 1.6720e-02,  2.7159e-01,  4.0920e-01,  ..., -4.2283e+00,\n           -4.2807e+00, -4.3174e+00],\n          [ 1.2366e+00,  2.1979e+00,  2.3827e+00,  ..., -3.8260e+00,\n           -4.0648e+00, -3.9723e+00],\n          [ 1.1236e+00,  2.5594e+00,  2.7788e+00,  ..., -3.4492e+00,\n           -3.3779e+00, -3.6755e+00]]]], dtype=torch.float64)\n22050\ntorch.Size([2, 10, 96, 64])\n","output_type":"stream"}]},{"cell_type":"code","source":"# torch.hub.help('harritaylor/torchvggish', 'vggish')\n# urls = {\n#             'vggish': \"https://github.com/harritaylor/torchvggish/releases/download/v0.1/vggish-10086976.pth\",\n#             'pca': \"vggish_pca_params-970ea276.pth\"\n#         }\n# vggish_model = vggish.VGGish(urls, preprocess=False)\n# print(vggish_model)\n# vggish_model.eval()\n# embedding = vggish_model.forward(vgg_data)\n# print(embedding)\n# print(embedding.shape)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T02:01:55.838617Z","iopub.execute_input":"2023-04-18T02:01:55.839353Z","iopub.status.idle":"2023-04-18T02:01:55.844612Z","shell.execute_reply.started":"2023-04-18T02:01:55.839316Z","shell.execute_reply":"2023-04-18T02:01:55.843425Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"torch.hub.help('harritaylor/torchvggish', 'vggish')","metadata":{"execution":{"iopub.status.busy":"2023-04-18T02:01:55.846199Z","iopub.execute_input":"2023-04-18T02:01:55.846658Z","iopub.status.idle":"2023-04-18T02:01:56.086843Z","shell.execute_reply.started":"2023-04-18T02:01:55.846621Z","shell.execute_reply":"2023-04-18T02:01:56.085682Z"},"trusted":true},"execution_count":115,"outputs":[{"name":"stderr","text":"Using cache found in /root/.cache/torch/hub/harritaylor_torchvggish_master\n","output_type":"stream"}]},{"cell_type":"code","source":"# Data splits\nbatch_size = 16\nmel_train_size = int(0.8*len(mel_spec_dataset))\nmel_test_size = len(mel_spec_dataset) - mel_train_size\n\nmel_train_set, mel_test_set = torch.utils.data.random_split(mel_spec_dataset, [mel_train_size, mel_test_size])\nmel_val_size = int(0.25*mel_test_size)\nmel_test_size = mel_test_size - mel_val_size\nprint(mel_train_size)\n\nmel_val_set, mel_test_set = torch.utils.data.random_split(mel_test_set, [mel_val_size, mel_test_size])\n\nmel_train_loader = DataLoader(mel_train_set, batch_size=batch_size, shuffle=True, num_workers=8)\nmel_val_loader = DataLoader(mel_val_set, batch_size=batch_size, shuffle=True, num_workers=8)\nmel_test_loader = DataLoader(mel_test_set, batch_size=batch_size, shuffle=True, num_workers=8)","metadata":{"id":"dXp9m65ifk-6","outputId":"dd6e3c38-ac1d-4b04-de05-314a8d7ebd6e","execution":{"iopub.status.busy":"2023-04-18T02:30:44.361465Z","iopub.execute_input":"2023-04-18T02:30:44.362685Z","iopub.status.idle":"2023-04-18T02:30:44.380212Z","shell.execute_reply.started":"2023-04-18T02:30:44.362640Z","shell.execute_reply":"2023-04-18T02:30:44.378892Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"16000\n","output_type":"stream"}]},{"cell_type":"code","source":"# Data splits\nbatch_size = 16\nvgg_train_size = int(0.8*len(vgg_dataset))\nvgg_test_size = len(vgg_dataset) - vgg_train_size\n\nvgg_train_set, vgg_test_set = torch.utils.data.random_split(vgg_dataset, [vgg_train_size, vgg_test_size])\nvgg_val_size = int(0.25*vgg_test_size)\nvgg_test_size = vgg_test_size - vgg_val_size\nprint(vgg_train_size)\n\nvgg_val_set, vgg_test_set = torch.utils.data.random_split(vgg_test_set, [vgg_val_size, vgg_test_size])\n\nvgg_train_loader = DataLoader(vgg_train_set, batch_size=batch_size, shuffle=True, num_workers=8)\nvgg_val_loader = DataLoader(vgg_val_set, batch_size=batch_size, shuffle=True, num_workers=8)\nvgg_test_loader = DataLoader(vgg_test_set, batch_size=batch_size, shuffle=True, num_workers=8)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T02:18:59.160629Z","iopub.execute_input":"2023-04-18T02:18:59.161779Z","iopub.status.idle":"2023-04-18T02:18:59.180816Z","shell.execute_reply.started":"2023-04-18T02:18:59.161730Z","shell.execute_reply":"2023-04-18T02:18:59.179216Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"16000\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n","output_type":"stream"}]},{"cell_type":"code","source":"# Wide Network built from scratch\n# class MelSpecNetwork(nn.Module):\n#   def __init__(self):\n#     super(MelSpecNetwork, self).__init__()\n    \n#     self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n#     self.relu1 = nn.ReLU()\n#     self.conv2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n#     self.relu2 = nn.ReLU()\n#     self.conv3 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n#     self.relu3 = nn.ReLU()\n#     self.conv4 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n#     self.relu4 = nn.ReLU()\n#     # 1st Arg: num channels (16) * width of spec (128) * length of spec (431)\n#     # 2nd Arg: num instrument classes (20)\n#     self.fc1 = nn.Linear(16*128*431, 20)\n#     self.sigmoid = nn.Sigmoid()\n\n#   def forward(self, input):\n#     output = self.conv1(input)\n#     output = self.relu1(output)\n#     output = self.conv2(output)\n#     output = self.relu2(output)\n#     # num channels (16) * width of spec (128) * length of spec (431)\n#     output = output.view(-1, 16*128*431)\n#     output = self.fc1(output)\n#     output = self.sigmoid(output)\n\n#     return output\n\n# Resnet50 Arch\n# class MelSpecNetwork(nn.Module):\n#   def __init__(self):\n#     super(MelSpecNetwork, self).__init__()\n\n#     self.resnet = models.resnet50(pretrained=True)\n#     default_in_ftrs = self.resnet.fc.in_features\n#     for param in self.resnet.parameters():\n#         param.requires_grad = False\n    \n#     # Don't freeze last layer of resnet\n# #     for param in self.resnet.layer4.parameters():\n# #         param.requires_grad = True\n\n#     # Replace fully connected layer to fit 20 instrument classes\n#     self.resnet.fc = nn.Linear(default_in_ftrs, 20)\n    \n\n#   def forward(self, input):\n#     output = self.resnet(input)\n\n#     return output\n\n\n# VGGish Arch\nclass MelSpecNetwork(nn.Module):\n  def __init__(self):\n    super(MelSpecNetwork, self).__init__()\n    \n    urls = {\n            'vggish': \"https://github.com/harritaylor/torchvggish/releases/download/v0.1/vggish-10086976.pth\",\n            'pca': \"vggish_pca_params-970ea276.pth\"\n        }\n    model = torch.hub.load('harritaylor/torchvggish', 'vggish', preprocess=False, postprocess=False)\n    self.vggish_model = torch.hub.load('harritaylor/torchvggish', 'vggish', preprocess=False, postprocess=False)\n    \n#     for param in self.vggish_model.parameters():\n#         param.requires_grad = False\n#     for layer_index, layer in enumerate(self.vggish_model.children()):\n#         print(layer_index)\n#         if layer_index == 13:\n#             for param in layer.parameters():\n#                 param.requires_grad = True\n#     for layer_index, layer in enumerate(self.vggish_model.features.children()):\n#         print(layer_index)\n#         if layer_index == 11 or layer_index == 13:\n#             for param in layer.parameters():\n#                 param.requires_grad = True\n\n#     for layer_index, layer in enumerate(self.vggish_model.embeddings.children()):\n#         if layer_index == 2 or layer_index == 4:\n#             for param in layer.parameters():\n#                 param.requires_grad = True\n\n\n#     for param in self.vggish_model.embeddings.parameters():\n#         param.requires_grad = True\n    \n    self.classify = nn.Sequential(\n            nn.Linear(512, 20),\n        )   \n\n  def forward(self, input):\n    bs, num_frames, _, _ = input.size()\n    input = input.view(bs*num_frames, 1, input.size(2), input.size(3))\n    vggish_logits = self.vggish_model(input) # [bs*num_frames, 128]\n    vggish_logits = vggish_logits.reshape(bs, vggish_logits.size(1) * num_frames)\n    \n    output = self.classify(vggish_logits)\n    \n    output = self.classify(vggish_logits)\n\n    return output","metadata":{"id":"zRELT8HXjMJQ","execution":{"iopub.status.busy":"2023-04-18T03:12:23.179559Z","iopub.execute_input":"2023-04-18T03:12:23.180605Z","iopub.status.idle":"2023-04-18T03:12:23.194157Z","shell.execute_reply.started":"2023-04-18T03:12:23.180554Z","shell.execute_reply":"2023-04-18T03:12:23.193022Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"mel_spec_net = MelSpecNetwork()\nprint(mel_spec_net)\nprint(sum(param.numel() for param in mel_spec_net.parameters() if param.requires_grad))","metadata":{"id":"bQ5A7m6dA2fL","outputId":"0aa87bea-d709-4ced-aaec-63e44a1219eb","execution":{"iopub.status.busy":"2023-04-18T03:12:27.558893Z","iopub.execute_input":"2023-04-18T03:12:27.559862Z","iopub.status.idle":"2023-04-18T03:12:28.796340Z","shell.execute_reply.started":"2023-04-18T03:12:27.559807Z","shell.execute_reply":"2023-04-18T03:12:28.795053Z"},"trusted":true},"execution_count":103,"outputs":[{"name":"stderr","text":"Using cache found in /root/.cache/torch/hub/harritaylor_torchvggish_master\n","output_type":"stream"},{"name":"stdout","text":"MelSpecNetwork(\n  (vggish_model): Sequential(\n    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (12): ReLU(inplace=True)\n    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (14): ReLU(inplace=True)\n    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classify): Sequential(\n    (0): Linear(in_features=512, out_features=20, bias=True)\n  )\n)\n4509972\n","output_type":"stream"}]},{"cell_type":"code","source":"mel_spec_net.eval()\nmel_spec_net.double()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmel_spec_net.to(device)\nmel_spec_net(next(iter(mel_train_loader))[\"specs\"].to(device))","metadata":{"id":"NEKEHA1eA6b7","outputId":"bd04ade5-c882-4129-e5c4-8c68e6deec3e","execution":{"iopub.status.busy":"2023-04-18T03:12:32.222600Z","iopub.execute_input":"2023-04-18T03:12:32.223148Z","iopub.status.idle":"2023-04-18T03:12:34.090522Z","shell.execute_reply.started":"2023-04-18T03:12:32.223103Z","shell.execute_reply":"2023-04-18T03:12:34.081686Z"},"trusted":true},"execution_count":104,"outputs":[{"execution_count":104,"output_type":"execute_result","data":{"text/plain":"tensor([[-7.4161e-03, -4.0460e-02,  8.8254e-03, -4.1167e-02,  2.8135e-02,\n         -1.7568e-02,  2.1229e-02, -2.3866e-03, -3.2883e-02,  2.7282e-02,\n          5.4677e-03, -4.7778e-02, -1.2149e-02, -2.7916e-02,  4.3513e-02,\n         -5.0747e-02, -2.1087e-02, -8.9301e-02,  1.6601e-02,  2.4027e-03],\n        [-1.8697e-02, -1.5853e-02, -2.3463e-03, -3.0159e-02,  1.8379e-02,\n         -4.4711e-02,  2.4290e-02,  1.7742e-03, -2.0696e-02,  3.9279e-02,\n         -3.3661e-03, -3.9283e-02, -2.4907e-03, -6.7596e-03,  5.7836e-02,\n         -6.9187e-02, -1.5523e-02, -9.1903e-02,  2.7232e-02, -3.3894e-02],\n        [-1.1220e-02, -3.1385e-02,  1.3983e-03, -2.6067e-02,  4.0275e-02,\n         -1.7953e-02,  1.9516e-02,  6.0581e-03, -2.1701e-02,  2.4186e-02,\n         -4.2735e-04, -4.2169e-02, -1.9722e-02, -1.8266e-02,  5.3457e-02,\n         -6.6389e-02, -1.4152e-02, -9.2399e-02,  1.0715e-02, -1.4644e-02],\n        [ 1.0809e-03, -3.7004e-02,  1.9590e-03, -3.6919e-02,  3.5853e-02,\n         -2.6803e-02,  2.8064e-02,  1.4549e-03, -3.0165e-02,  4.0804e-02,\n         -2.1322e-03, -4.4252e-02, -4.5929e-03, -2.8308e-02,  5.0261e-02,\n         -5.5376e-02, -1.1681e-02, -9.3182e-02,  1.9566e-02, -5.3782e-03],\n        [-8.4067e-03, -1.6203e-02,  2.0868e-02, -3.8293e-02,  1.8790e-02,\n         -1.4381e-02,  2.8389e-02, -1.5865e-02, -2.0806e-02,  3.0733e-02,\n         -1.2437e-02, -5.0510e-02,  1.5839e-02,  8.9247e-04,  4.1839e-02,\n         -3.3153e-02, -2.7091e-02, -7.2938e-02,  2.5074e-02, -7.7171e-03],\n        [-1.8689e-02, -2.3015e-02,  3.7713e-03, -3.4415e-02,  1.9283e-02,\n         -2.0590e-02,  1.8261e-02, -1.1254e-02, -1.6858e-02,  3.6804e-02,\n          9.5572e-04, -4.2298e-02,  5.9308e-03, -7.2547e-03,  4.6203e-02,\n         -5.3025e-02, -2.2945e-02, -7.9629e-02,  2.3669e-02, -1.4184e-02],\n        [-1.6071e-02, -1.3327e-02,  1.6222e-02, -2.5016e-02,  5.1987e-03,\n         -6.5384e-03,  1.3360e-02, -2.4959e-02, -9.2394e-03,  2.1712e-02,\n          1.7197e-03, -2.7717e-02,  2.0423e-02,  2.0188e-02,  4.0314e-02,\n         -3.3977e-02, -3.6047e-02, -5.4839e-02,  3.1143e-02, -4.4823e-04],\n        [-1.2527e-03, -5.5133e-02,  2.4940e-03, -4.6216e-02,  4.4290e-02,\n         -1.6645e-02,  2.6465e-02,  2.8566e-03, -5.2897e-02,  2.4001e-02,\n          6.8138e-04, -5.5348e-02, -1.6746e-02, -4.5326e-02,  3.3832e-02,\n         -5.4019e-02, -1.4755e-02, -9.3285e-02,  2.6372e-03,  1.8028e-02],\n        [ 1.1985e-02, -4.2532e-02, -1.4483e-02, -3.9009e-02,  4.3875e-02,\n         -2.5537e-02,  2.7312e-02,  1.1822e-02, -3.6336e-02,  4.0754e-02,\n         -2.8406e-04, -5.3164e-02, -1.3543e-02, -4.9337e-02,  5.0999e-02,\n         -6.0263e-02, -7.2085e-03, -1.0571e-01,  6.3625e-03,  6.5270e-03],\n        [-6.2115e-03, -2.4716e-02,  4.4605e-03, -2.9395e-02,  2.7936e-02,\n         -1.8587e-02,  2.3707e-02, -8.7489e-03, -2.1951e-02,  3.6866e-02,\n         -1.2963e-02, -4.5542e-02, -5.8182e-04, -9.8073e-03,  5.4452e-02,\n         -5.2365e-02, -1.8107e-02, -8.7743e-02,  2.3081e-02, -9.4201e-03],\n        [-1.2187e-02, -1.4114e-02,  1.4530e-02, -3.3343e-02,  1.8932e-02,\n         -2.4481e-02,  2.9745e-02, -5.5623e-03, -1.0251e-02,  4.2832e-02,\n         -2.2456e-02, -2.8466e-02, -9.3134e-06,  4.5012e-03,  5.6311e-02,\n         -5.6538e-02, -2.1678e-02, -8.2471e-02,  3.8202e-02, -1.9067e-02],\n        [-4.5568e-03, -4.0480e-02, -8.8824e-03, -4.2626e-02,  3.6306e-02,\n         -2.0171e-02,  2.2945e-02,  8.9779e-03, -4.6097e-02,  2.9014e-02,\n         -3.1549e-03, -5.6609e-02, -1.4318e-02, -4.0909e-02,  4.3766e-02,\n         -6.5192e-02, -9.4185e-03, -9.7888e-02,  2.1720e-03,  2.9043e-03],\n        [-8.4337e-03, -2.8886e-02,  7.7402e-04, -3.8523e-02,  2.9452e-02,\n         -2.0995e-02,  2.3825e-02, -4.7737e-03, -2.4031e-02,  3.3132e-02,\n         -6.7092e-03, -4.1784e-02, -7.8394e-04, -1.2597e-02,  4.9614e-02,\n         -5.2596e-02, -1.2592e-02, -8.7981e-02,  2.1453e-02, -8.7987e-03],\n        [-2.1918e-03, -2.8811e-02, -2.5132e-04, -3.7601e-02,  2.7305e-02,\n         -3.0005e-02,  2.3033e-02, -6.1520e-04, -2.7016e-02,  3.8365e-02,\n         -7.6125e-03, -4.2674e-02, -2.8402e-03, -2.5926e-02,  5.6241e-02,\n         -5.5980e-02, -1.2925e-02, -1.0036e-01,  1.7372e-02, -1.2857e-02],\n        [ 3.7932e-03, -4.6268e-02, -9.3423e-03, -4.2542e-02,  4.6229e-02,\n         -1.8409e-02,  2.5892e-02,  1.2652e-02, -5.0873e-02,  2.3808e-02,\n         -3.6036e-03, -5.7020e-02, -2.2880e-02, -4.5174e-02,  4.9012e-02,\n         -6.2813e-02, -4.3921e-03, -1.0238e-01, -5.2147e-03,  7.7236e-03],\n        [-9.3561e-03, -2.3972e-02, -3.3948e-03, -3.6531e-02,  2.3372e-02,\n         -3.1286e-02,  2.5369e-02,  3.5425e-03, -3.1691e-02,  4.0628e-02,\n         -3.4217e-03, -4.6126e-02, -8.3257e-04, -2.0927e-02,  4.6301e-02,\n         -6.1580e-02, -1.3792e-02, -9.1146e-02,  1.7618e-02, -1.5499e-02]],\n       device='cuda:0', dtype=torch.float64, grad_fn=<AddmmBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"mel_spec_net.eval().double()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmel_spec_net.to(device)\nprint(next(mel_spec_net.parameters()).is_cuda)\ndata = next(iter(vgg_train_loader))[\"specs\"]\ndata = data.to(device)\nprint(data.device)\n\nresult = mel_spec_net(data)\nprint(result.shape)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T02:19:21.940650Z","iopub.execute_input":"2023-04-18T02:19:21.941045Z","iopub.status.idle":"2023-04-18T02:19:27.438340Z","shell.execute_reply.started":"2023-04-18T02:19:21.941009Z","shell.execute_reply":"2023-04-18T02:19:27.436707Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"True\ncuda:0\ntorch.Size([16, 20])\n","output_type":"stream"}]},{"cell_type":"code","source":"class EarlyStopping():\n  def __init__(self, spec_type, version, patience=7, verbose=True, delta=0):\n    self.spec_type = spec_type\n    self.version = version\n    self.patience = patience\n    self.verbose = verbose\n    self.counter = 0\n    self.best_score = None\n    self.early_stop = False\n    self.val_loss_min = np.Inf\n    self.delta = delta\n\n  def __call__(self, val_loss, epoch, model, optimizer):\n        score = -val_loss\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, epoch, model, optimizer)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, epoch, model, optimizer)\n            self.counter = 0\n\n  def save_checkpoint(self, val_loss, epoch, model, optimizer):\n        \"\"\"Saves model when validation loss decrease.\"\"\"\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        whole_state = {'epoch': epoch + 1, 'model':model.state_dict(), 'optimizer':optimizer.state_dict(), 'val_loss':val_loss}\n        torch.save(whole_state, os.path.join('/kaggle/working', self.spec_type + \n                                             '_v' + str(self.version) + '.pt'))\n        self.val_loss_min = val_loss","metadata":{"id":"Q_dJ5HovXfbO","execution":{"iopub.status.busy":"2023-04-18T02:33:43.428623Z","iopub.execute_input":"2023-04-18T02:33:43.429696Z","iopub.status.idle":"2023-04-18T02:33:43.440809Z","shell.execute_reply.started":"2023-04-18T02:33:43.429649Z","shell.execute_reply":"2023-04-18T02:33:43.439584Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"class MelSpecTrainer():\n  def __init__(self, model, num_epochs, train_loader, val_loader, spec_type, version, lr=1e-6, weight_decay=0.0001):\n    # Define your execution device\n    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    print(\"The model will be running on\", self.device, \"device\")\n    self.model = model\n    self.num_epochs = num_epochs\n    self.train_loader = train_loader\n    self.val_loader = val_loader\n    self.spec_type = spec_type\n    self.version = version\n    # Convert model parameters and buffers to CPU or Cuda\n    self.model.to(self.device)\n\n    # Model parameters should be doubles to match dataset sample datatype\n    self.model.double()\n\n    self.loss_fn = nn.BCEWithLogitsLoss()\n#     self.optimizer = torch.optim.Adam(\n#         [\n#         {\"params\": self.model.resnet.layer1.parameters(), \"lr\": 1e-5},\n#         {\"params\": self.model.resnet.layer2.parameters(), \"lr\": 1e-5},\n#         {\"params\": self.model.resnet.layer3.parameters(), \"lr\": 1e-5},\n#         {\"params\": self.model.resnet.layer4.parameters(), \"lr\": 1e-5},\n#         {\"params\": self.model.resnet.fc.parameters(), \"lr\":lr}]\n#         , lr=lr)\n#     self.optimizer = torch.optim.Adam(\n#         [\n#         {\"params\": self.model.vggish_model.features.parameters(), \"lr\": 1e-5},\n#         {\"params\": self.model.vggish_model.embeddings.parameters(), \"lr\": 1e-5},\n#         {\"params\": self.model.classify.parameters(), \"lr\":lr}]\n#         , lr=lr, weight_decay=weight_decay)\n    self.optimizer = torch.optim.Adam(\n        [\n        {\"params\": self.model.vggish_model.parameters(), \"lr\": 1e-5},\n        {\"params\": self.model.classify.parameters(), \"lr\":lr}]\n        , lr=lr, weight_decay=weight_decay)\n    self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n    self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=6, gamma=0.1)\n\n    self.early_stop = EarlyStopping(self.spec_type, self.version, patience=20)\n\n  def continue_train(self, state_dict_name, extra_epochs=0):\n    state_dict = torch.load('/kaggle/working/' + state_dict_name, map_location=self.device)\n    model_sd = state_dict['model']\n    optimizer_sd = state_dict['optimizer']\n    epochs_done = state_dict['epoch']\n    min_val_loss = state_dict['val_loss']\n    print(epochs_done)\n    self.model.load_state_dict(model_sd)\n    self.optimizer.load_state_dict(optimizer_sd)\n    self.num_epochs = self.num_epochs - epochs_done + extra_epochs\n    self.early_stop(min_val_loss, epochs_done - 1, self.model, self.optimizer)\n\n  def train(self):\n    print(self.device)\n    self.model.train()\n    pbar = master_bar(range(self.num_epochs))\n    headers = ['Train_Loss', 'Val_Loss', 'F1-Macro', 'F1-Micro', 'JS', 'Time']\n    pbar.write(headers, table=True)\n    train_size = len(self.train_loader.dataset)\n    for epoch in pbar:  # loop over the dataset multiple times\n      running_train_loss = 0.0\n      for i, sample in enumerate(progress_bar(self.train_loader, parent=pbar)):\n        specs = sample['specs']\n        labels = sample['instrument(s)']\n        \n        num_rows = specs.shape[0]\n\n        # Labels are 1 x num of instrument classes\n        # Need to remove the 1 dimension to get a batch size x num instrument classes result\n        labels = labels.squeeze(dim=1)\n\n        # Optionally print label\n        # print(list(type_map.keys())[list(type_map.values()).index(labels)])\n\n        # get the inputs\n        specs = Variable(specs.to(self.device))\n        labels = Variable(labels.to(self.device))\n\n        # zero the parameter gradients\n        self.optimizer.zero_grad()\n        # predict classes using images from the training set\n        outputs = self.model(specs)\n        \n        # compute the loss based on model output and real labels\n        loss = self.loss_fn(outputs, labels)\n        running_train_loss += loss * num_rows\n        # backpropagate the loss\n        loss.backward()\n        # adjust parameters based on the calculated gradients\n        self.optimizer.step()\n      self.scheduler.step()\n      running_train_loss = running_train_loss / train_size\n      print(epoch)\n      print(running_train_loss)\n      overall_val_loss, pred_dict = self.predict(pbar)\n      y_true, y_pred = pred_dict['y_true'], pred_dict['y_pred']\n\n      str_stats = []\n      stats = [running_train_loss,\n                overall_val_loss,\n                f1_score(y_true, y_pred, average=\"macro\"),\n                f1_score(y_true, y_pred, average=\"micro\"),\n                jaccard_score(y_true, y_pred, average=\"samples\")]\n\n      for stat in stats:\n          str_stats.append(\n              'NA' if stat is None else str(stat) if isinstance(stat, int) else f'{stat:.4f}'\n          )\n\n      pbar.write(str_stats, table=True)\n      val_loss = self.predict(pbar)\n      self.early_stop(overall_val_loss, epoch, self.model, self.optimizer)\n      if self.early_stop.early_stop:\n          print(\"Early stopping\")\n          break\n\n  def predict(self, pbar=None, threshold=0.5):\n    \"\"\"\n    Evaluate the model on a validation set\n    :param device: str (defaults to 'cpu')\n    :param pbar: fast_progress progress bar (defaults to None)\n    :returns: overall_val_loss (float), accuracies (dict{'acc': value}, preds (dict)\n    \"\"\"\n    val_size = len(self.val_loader.dataset)\n    # Size is 28 for the 28 emotion classes in GoEmotions dataset\n    running_val_loss = 0.0\n    self.model.to(self.device)\n    self.model.eval()\n\n    preds_dict = {\n            'y_true': np.zeros([val_size, 20]),\n            'y_pred': np.zeros([val_size, 20])\n        }\n\n    with torch.no_grad():\n      index_dict = 0\n      for i, sample in enumerate(progress_bar(self.val_loader, parent=pbar, leave=(pbar is not None))):\n        specs = sample['specs']\n        labels = sample['instrument(s)']\n        labels = labels.squeeze(dim=1)\n\n        num_rows = specs.shape[0]\n\n        specs = Variable(specs.to(self.device))\n        labels = Variable(labels.to(self.device))\n\n        y_prob = self.model(specs)\n        running_val_loss += self.loss_fn(y_prob, labels) * num_rows\n\n        y_pred = (torch.sigmoid(y_prob) > threshold).float().cpu().numpy()\n\n        current_index = index_dict\n        preds_dict['y_true'][current_index: current_index + num_rows, :] = labels.cpu().detach().numpy()\n        preds_dict['y_pred'][current_index: current_index + num_rows, :] = y_pred\n        index_dict += num_rows\n\n    running_val_loss = running_val_loss / val_size\n\n    return running_val_loss, preds_dict\n\n  def saveModel(self, model, epoch_num, version):\n    path = '/kaggle/working/' + self.spec_type + \"_v\" + str(version) + \"_epoch\" + str(epoch_num) + '.pth'\n    torch.save(model.state_dict(), path)","metadata":{"id":"dlEFK08EL8KS","execution":{"iopub.status.busy":"2023-04-18T03:12:38.295218Z","iopub.execute_input":"2023-04-18T03:12:38.295646Z","iopub.status.idle":"2023-04-18T03:12:38.336802Z","shell.execute_reply.started":"2023-04-18T03:12:38.295602Z","shell.execute_reply":"2023-04-18T03:12:38.335591Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"num_epochs = 30\nmel_spec_trainer = MelSpecTrainer(mel_spec_net, num_epochs, mel_train_loader, mel_val_loader, 'mel_spectrogram', 4, lr=1e-3)","metadata":{"id":"xmhhAz3NSCrR","outputId":"8124f6f9-5dc6-49a2-a115-a00447280ca9","execution":{"iopub.status.busy":"2023-04-18T03:12:43.059665Z","iopub.execute_input":"2023-04-18T03:12:43.060736Z","iopub.status.idle":"2023-04-18T03:12:43.069150Z","shell.execute_reply.started":"2023-04-18T03:12:43.060697Z","shell.execute_reply":"2023-04-18T03:12:43.067916Z"},"trusted":true},"execution_count":106,"outputs":[{"name":"stdout","text":"The model will be running on cuda:0 device\n","output_type":"stream"}]},{"cell_type":"code","source":"num_epochs = 30\nmel_spec_trainer = MelSpecTrainer(mel_spec_net, num_epochs, vgg_train_loader, vgg_val_loader, 'vgg', 4, lr=1e-2)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T02:32:07.687918Z","iopub.execute_input":"2023-04-18T02:32:07.688306Z","iopub.status.idle":"2023-04-18T02:32:07.697261Z","shell.execute_reply.started":"2023-04-18T02:32:07.688271Z","shell.execute_reply":"2023-04-18T02:32:07.695966Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"The model will be running on cuda:0 device\n","output_type":"stream"}]},{"cell_type":"code","source":"mel_spec_trainer.train()","metadata":{"id":"4cEUDCbWR7XY","outputId":"766db431-50dd-4b49-87b4-40d147a92274","execution":{"iopub.status.busy":"2023-04-18T03:12:45.530708Z","iopub.execute_input":"2023-04-18T03:12:45.531297Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"cuda:0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      <progress value='5' class='' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      16.67% [5/30 2:30:50&lt;12:34:12]\n    </div>\n    \n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Train_Loss</th>\n      <th>Val_Loss</th>\n      <th>F1-Macro</th>\n      <th>F1-Micro</th>\n      <th>JS</th>\n      <th>Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0.3232</td>\n      <td>0.3134</td>\n      <td>0.0418</td>\n      <td>0.0575</td>\n      <td>0.0340</td>\n    </tr>\n    <tr>\n      <td>0.3072</td>\n      <td>0.3042</td>\n      <td>0.0541</td>\n      <td>0.0747</td>\n      <td>0.0325</td>\n    </tr>\n    <tr>\n      <td>0.2971</td>\n      <td>0.2891</td>\n      <td>0.1355</td>\n      <td>0.1505</td>\n      <td>0.0846</td>\n    </tr>\n    <tr>\n      <td>0.2877</td>\n      <td>0.2843</td>\n      <td>0.1332</td>\n      <td>0.1450</td>\n      <td>0.0801</td>\n    </tr>\n    <tr>\n      <td>0.2817</td>\n      <td>0.2801</td>\n      <td>0.1876</td>\n      <td>0.2082</td>\n      <td>0.1104</td>\n    </tr>\n    <tr>\n      <td>0.2767</td>\n      <td>0.2762</td>\n      <td>0.1910</td>\n      <td>0.2112</td>\n      <td>0.1179</td>\n    </tr>\n  </tbody>\n</table><p>\n\n    <div>\n      <progress value='16' class='' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      25.40% [16/63 00:09&lt;00:28]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"0\ntensor(0.3232, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n","output_type":"stream"},{"name":"stdout","text":"Validation loss decreased (inf --> 0.313394).  Saving model ...\n1\ntensor(0.3072, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n","output_type":"stream"},{"name":"stdout","text":"Validation loss decreased (0.313394 --> 0.304201).  Saving model ...\n2\ntensor(0.2971, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n","output_type":"stream"},{"name":"stdout","text":"Validation loss decreased (0.304201 --> 0.289070).  Saving model ...\n3\ntensor(0.2877, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n","output_type":"stream"},{"name":"stdout","text":"Validation loss decreased (0.289070 --> 0.284341).  Saving model ...\n4\ntensor(0.2817, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n","output_type":"stream"},{"name":"stdout","text":"Validation loss decreased (0.284341 --> 0.280085).  Saving model ...\n5\ntensor(0.2767, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n","output_type":"stream"}]},{"cell_type":"code","source":"mel_spec_trainer.continue_train('mel_spectrogram_v4.pt', 10)","metadata":{"id":"fqwL9FkvR_er","outputId":"79677dde-168b-4279-f71d-e63ae5bbd4ba","execution":{"iopub.status.busy":"2023-04-18T02:01:58.564344Z","iopub.status.idle":"2023-04-18T02:01:58.564940Z","shell.execute_reply.started":"2023-04-18T02:01:58.564673Z","shell.execute_reply":"2023-04-18T02:01:58.564698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mel_spec_trainer.train()","metadata":{"id":"-CpoGlYkIjL4","outputId":"ada8eaf0-ac96-4fa8-cc47-7910a78dc883","execution":{"iopub.status.busy":"2023-04-18T02:01:58.566576Z","iopub.status.idle":"2023-04-18T02:01:58.567433Z","shell.execute_reply.started":"2023-04-18T02:01:58.567183Z","shell.execute_reply":"2023-04-18T02:01:58.567209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MelSpecTester():\n  def __init__(self, model, state_dict_name):\n    self.model = model\n    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    self.state_dict = torch.load('/kaggle/working/' + state_dict_name, map_location=self.device)\n    self.model_sd = self.state_dict['model']\n    self.model.load_state_dict(self.model_sd)\n\n  def test_accuracy(self, test_loader, threshold):\n      \"\"\"\n      Evaluate the model on a validation set\n      :param device: str (defaults to 'cpu')\n      :param pbar: fast_progress progress bar (defaults to None)\n      :returns: overall_val_loss (float), accuracies (dict{'acc': value}, preds (dict)\n      \"\"\"\n      num_correct = 0\n      num_incorrect = 0\n      test_size = len(test_loader.dataset)\n      self.model.to(self.device)\n      self.model.eval()\n\n      preds_dict = {\n              'y_true': np.zeros([test_size, 20]),\n              'y_pred': np.zeros([test_size, 20])\n          }\n\n      with torch.no_grad():\n        index_dict = 0\n        for i, sample in enumerate(progress_bar(test_loader)):\n          specs = sample['specs']\n          labels = sample['instrument(s)']\n          labels = labels.squeeze(dim=1)\n\n          num_rows = specs.shape[0]\n\n          specs = Variable(specs.to(self.device))\n          labels = Variable(labels.to(self.device))\n\n          y_prob = self.model(specs)\n\n\n          y_pred = (torch.sigmoid(y_prob) > threshold).float().cpu().numpy()\n          labels = labels.cpu().detach().numpy()\n\n          current_index = index_dict\n          preds_dict['y_true'][current_index: current_index + num_rows, :] = labels\n          preds_dict['y_pred'][current_index: current_index + num_rows, :] = y_pred\n          index_dict += num_rows\n          batch_correct = np.count_nonzero((y_pred == labels).astype(int)) # + torch.count_nonzero((predicted == labels_2).int())\n          num_correct += batch_correct\n          num_incorrect += specs.shape[0] * 20 - batch_correct\n            \n      y_true, y_pred = preds_dict['y_true'], preds_dict['y_pred']\n\n      str_stats = []\n      stats = [f1_score(y_true, y_pred, average=\"macro\"),\n                f1_score(y_true, y_pred, average=\"micro\"),\n                jaccard_score(y_true, y_pred, average=\"samples\")]\n\n      for stat in stats:\n          str_stats.append(\n              'NA' if stat is None else str(stat) if isinstance(stat, int) else f'{stat:.4f}'\n          )\n      print(str_stats)\n      \n      print(\"Correctly Labeled: \" + str(num_correct))\n      print(\"Incorrectly Labeled: \" + str(num_incorrect))\n      return preds_dict\n","metadata":{"id":"y1EIGi2TP547","execution":{"iopub.status.busy":"2023-04-18T02:01:58.569243Z","iopub.status.idle":"2023-04-18T02:01:58.569908Z","shell.execute_reply.started":"2023-04-18T02:01:58.569631Z","shell.execute_reply":"2023-04-18T02:01:58.569657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mel_spec_tester = MelSpecTester(mel_spec_net, 'mel_spectrogram_v4.pt')\nmel_spec_tester.test_accuracy(mel_test_loader, 0.5)","metadata":{"id":"ebzaDuYwt0k0","outputId":"0776e301-c0dc-405b-a191-e74725e7c924","execution":{"iopub.status.busy":"2023-04-18T02:01:58.571534Z","iopub.status.idle":"2023-04-18T02:01:58.573127Z","shell.execute_reply.started":"2023-04-18T02:01:58.572848Z","shell.execute_reply":"2023-04-18T02:01:58.572876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'/kaggle/working/vgg_v4.pt')","metadata":{"execution":{"iopub.status.busy":"2023-04-18T02:01:58.574599Z","iopub.status.idle":"2023-04-18T02:01:58.575443Z","shell.execute_reply.started":"2023-04-18T02:01:58.575179Z","shell.execute_reply":"2023-04-18T02:01:58.575206Z"},"trusted":true},"execution_count":null,"outputs":[]}]}